{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d462ee31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "814f7375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'utt_0001', 'text': 'my credit card number is 4242 4242 4242 4242 and my email is ramesh dot sharma at gmail dot com', 'entities': [{'start': 26, 'end': 49, 'label': 'CREDIT_CARD'}, {'start': 66, 'end': 80, 'label': 'PERSON_NAME'}, {'start': 84, 'end': 104, 'label': 'EMAIL'}]}\n",
      "[{'start': 26, 'end': 49, 'label': 'CREDIT_CARD'}, {'start': 66, 'end': 80, 'label': 'PERSON_NAME'}, {'start': 84, 'end': 104, 'label': 'EMAIL'}]\n",
      "4242 4242 4242 4242 and \n",
      "sh dot sharma a\n"
     ]
    }
   ],
   "source": [
    "data = \"../data/train.jsonl\"\n",
    "\n",
    "train = []\n",
    "with open(data, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        train.append(json.loads(line))\n",
    "\n",
    "print(train[0])\n",
    "print(train[0]['entities'])\n",
    "start = train[0]['entities'][0]['start']\n",
    "end = train[0]['entities'][0]['end']\n",
    "print(train[0]['text'][start-1: end])\n",
    "\n",
    "start = train[0]['entities'][1]['start']\n",
    "end = train[0]['entities'][1]['end']\n",
    "print(train[0]['text'][start-1: end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9262d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import create_model, create_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1666335e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec727a16264941c3aaba594f16c561e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e47a770af7924b9e9e98d6891979e8fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b47c09cb10ed4a70b533360288baed36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = create_model(model_name=\"google-bert/bert-base-uncased\")\n",
    "tokenizer = create_tokenizer(tokenizer_name=\"google-bert/bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b32cc3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import PIIDataset, collate_batch\n",
    "from labels import *\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "898849cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = LABELS\n",
    "dataset = PIIDataset(path=\"../data/train.jsonl\", tokenizer=tokenizer, label_list=label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c15f34cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(dataset, tokenizer, batch_size=1, shuffle=True):\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "    label_pad_id = -100\n",
    "\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        collate_fn=lambda batch: collate_batch(batch, pad_token_id, label_pad_id)\n",
    "    )\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a882d1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = create_dataloader(dataset, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "35b4bc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATES = [\n",
    "    \"my credit card number is {card} and my email is {email}\",\n",
    "    \"the card number is {card} and the name on it is {name}\",\n",
    "    \"my name is {name} and my phone number is {phone}\",\n",
    "    \"please update the account for {name}, email {email}, phone {phone}\",\n",
    "    \"{name}'s credit card {card} was declined yesterday\",\n",
    "    \"my phone number is {phone}, and my email is {email}\",\n",
    "]\n",
    "\n",
    "HOMOPHONE_MAP = {\n",
    "    \"four\": \"for\",\n",
    "    \"two\": \"too\",\n",
    "    \"to\": \"2\",\n",
    "    \"zero\": \"0\",\n",
    "    \"one\": \"1\",\n",
    "    \"three\": \"3\",\n",
    "    \"eight\": \"ate\",\n",
    "    \"at\": \"@\" ,\n",
    "    \"dot\": \".\",\n",
    "    \"and\": \"n\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "74149c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import names\n",
    "import faker\n",
    "import json\n",
    "\n",
    "fake = faker.Faker(\"en_IN\")\n",
    "\n",
    "def gen_card():\n",
    "    blocks = [str(random.randint(1000,9999)) for _ in range(4)]\n",
    "    return \" \".join(blocks)\n",
    "\n",
    "def gen_email(name):\n",
    "    parts = name.lower().split()\n",
    "    return f\"{parts[0]}.{parts[-1]}@{fake.free_email_domain()}\"\n",
    "\n",
    "def gen_phone():\n",
    "    return fake.msisdn()[0:10]\n",
    "\n",
    "def homophone_noise(text):\n",
    "    words = text.split()\n",
    "    noisy_words = []\n",
    "    for w in words:\n",
    "        lw = w.lower()\n",
    "        if lw in HOMOPHONE_MAP and random.random() < 0.3:\n",
    "            noisy_words.append(HOMOPHONE_MAP[lw])\n",
    "        else:\n",
    "            noisy_words.append(w)\n",
    "    return \" \".join(noisy_words)\n",
    "\n",
    "def apply_asr_noise(text):\n",
    "    text = homophone_noise(text)\n",
    "    return text.lower()\n",
    "\n",
    "def create_example(template):\n",
    "    name = names.get_full_name()\n",
    "    card = gen_card()\n",
    "    email = gen_email(name)\n",
    "    phone = gen_phone()\n",
    "\n",
    "    text = template.format(name=name, card=card, email=email, phone=phone)\n",
    "    clean_text = text\n",
    "\n",
    "    noisy_text = apply_asr_noise(clean_text)\n",
    "\n",
    "    entities = []\n",
    "    for label, value in [(\"PERSON_NAME\", name.lower()), (\"CREDIT_CARD\", card), (\"EMAIL\", email), (\"PHONE\", phone)]:\n",
    "        start = noisy_text.find(value.lower())\n",
    "        if start != -1:\n",
    "            entities.append({\"start\": start, \"end\": start + len(value), \"label\": label})\n",
    "\n",
    "    return {\"id\": fake.uuid4(), \"text\": noisy_text, \"entities\": entities}\n",
    "\n",
    "def generate_dataset(n=500, out_path=\"synthetic.jsonl\"):\n",
    "    with open(out_path, \"w\") as f:\n",
    "        for _ in range(n):\n",
    "            tpl = random.choice(TEMPLATES)\n",
    "            ex = create_example(tpl)\n",
    "            f.write(json.dumps(ex) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bc24f419",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_dataset(n=1000, out_path=\"train.jsonl\")\n",
    "generate_dataset(n=200, out_path=\"dev.jsonl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
